# 실시간 주식 데이터 연계
<img src="/capture/pipeline_capture.png" width="1500"/>

## 실시간 주식 데이터 자동화 분석을 위한 파이프라인 제작 및 배치 
- 국내 주식의 실시간 체결가를 수집해서 누적 거래량, 누적 거래 대금, 거래량 가중 평균 가격, 체결강도, 매수/매도 압력을 기록
- 장 시작시간을 고려하여 평일 08:50분 시작, 16:00에 종료
- 안정성과 다른 추가 분석을 위한 원본 데이터 별도 수집
### 데이터 수집 및 적재(한국투자증권 api --> kafka, MongoDB)
<img src="/capture/1.data_extract/Screenshot_2.png" width="1000"/>
<img src="/capture/2.mongo_load/Screenshot_1.png" width="1000"/>

1. 삼성전자, SK하이닉스, 카카오 주식의 실시간 호가, 체결가, 예상체결가를 websockets 방식으로 수집, 직렬화 합니다.
2. 직렬화 한 데이터는 몽고DB 와 kafka 로 각각 보내집니다.
    - 몽고DB에 따로 적재하는 이유
    1. 원시데이터의 스키마가 일정하지 않음
    2. 짧은 시간 동안 대량의 데이터를 적재하기에 몽고DB 가 적절하다고 생각했습니다.
    3. 적재하는 원시데이터의 종류가 늘어나는 경우에도 유연한 확장성
    4. 이후 추가 분석이나 오류로 인한 검증 작업을 위해 윈시데이터를 적재했습니다.

### 데이터 추출 and 가공(kafka --> Spark)
1. 카프카의 데이터 중에 실시간 체결가를 스파크 스트리밍을 이용해서 실시간 구독합니다.
2. 오늘 날짜로 필터링해서 실시간으로 분석합니다.
### 분석 결과 리포트 적재(Spark --> AWS s3)
<img src="/capture/3.report_daily/Screenshot_1.png" width="1000"/>
<img src="/capture/3.report_daily/Screenshot_3.png" width="1000"/>

1. 집계한 결과는 aws s3 버킷에 저장됩니다.
2. daily_report/stock_code= 와 같이 종목 별로 디렉토리가 나뉘고 그 안에 yyyy-mm-dd.csv 형식의 단일 파일로 집계결과가 추가됩니다.
    - 실시간 리포트를 s3에 업데이트 및 .csv 을 선택한 이유 
    1. 결과 리포트를 사람이 직접 눈으로 확인하기 위해서는 .csv 형식이 적절하다고 생각했습니다.
    2. 실시간 데이터라는 특성에 맞게 s3에 실시간으로 공유하도록 했습니다.
    3. 이후 데이터양이 증가함에 따라 .parquet 형식으로 변경, 해당 결과리포트를 자바나 파이썬 등으로 한번에 불러오는 등 확장을 고려할 수 있습니다.
## 배치 작업(AirFlow)
<img src="/capture/4.quality_test_mongo/test_mongo_log.png" width="1000"/>
<img src="/capture/5.data_monitoring/email_report.png" width="1000"/>

1. 매주 평일 오전 08시 50분에 전체 배치 시작, 오후 04시 00분에 배치가 종료됩니다.
2. 16:05 데이터의 이상치를 포함한 품질 검사합니다.
3. 5분마다 데이터가 제대로 적재되고있는지 확인하고 만약 5분 이상 적재되지 않으면 알람 이메일을 보냅니다.
## 회고
### 개발환경
- 기존 아나콘다에서 환경 별로 관리하던 프로젝트를 EC2 로 변경하면서 로컬에서 개발하고 EC2 에 배포했음
    - 문제점
    1. 개발하고 베포하기까지 시간이 오래 걸림
    2. 로컬과 EC2 의 코드가 서로 다른 경우가 생김 - 디버깅 어려움  
        --> 로컬의 vscode 로 EC2 에 원격접속해서 해결
### 분산 시스템, 병렬 처리
- 카프카와 스파크를 이용해서 분산, 병렬처리를 할 수 있다고는 하지만 현재 다루는 데이터의 양이 많지 않고 EC2 서버 비용 문제로 m5.xlarge, RAM=16GB, CPU 수=4 를 사용하고 있기때문에 구현하지 않았음
- 파티션을 늘려서 처리하기에는 자원이 한정적이라 현재는 병렬처리로 인한 성능향상을 기대하기 어려움
- EC2 서버의 스팩을 높여서 병렬처리를 하거나 여러 대의 서버를 활용해서 분산 처리를 할 수 있다면 데이터가 늘어나도 수월하게 처리할 수 있을 것이라 생각함
- 원시데이터의 종류가 늘어나고 데이터도 한달, 1년 단위로 적재한다고 하면 몽고DB --> HDFS 로의 확장도 고려할 수 있음
### 실시간 데이터 확인(구현 예정)
- S3 에 저장되는 리포트가 실시간 추가 방식이기때문에 사람이 직접 파일을 열어서 확인하는 방식은 부적절함
    - 이를 실시간으로 읽어서 볼 수 있는(시각화) 방법이 필요
- 처리에 실패한 메시지
    - api 를 호출하기 때문에 가능성은 낮지만 처리에 실패하는 메시지가 생길 수 있음
    - DLQ(Dead Letter Queue) 를 도입해서 실패한 메시지를 모아 패턴을 분석해서 대응하는 과정이 필요